# one-hot编码

## 1. 什么是one-hot编码

​	one-hot 编码用于将离散的分类标签转换为二进制向量。

> [!IMPORTANT]
>
> 重点概念：离散分类；二进制向量

### 1.1 离散分类

​	就是将观察对象或数据实例分配到预定义的，互斥的，有限数量的类别中的过程。

主要特征：

- 有限类别：分类结果属于一组预定义的，数量有限的类别；
- 互斥性：每个观察对象只能属于一个类别，不能同时属于多个类别；
- 离散性：类别之间是明确分开的，相互独立的，没有中间状态或者连续过程；
- 非顺序性：类别之间通常没有固有的顺序或等级关系；

数学表达：可以表示为一个函数 f：X → Y，其中：

- X 是观察对象或实例的集合
- Y 是预定义类别的有限集合
- 对于任意 x ∈ X，存在唯一的 y ∈ Y，使得 f(x) = y

### 1.2 二进制向量

​	元素只有0和1的向量，称之为二进制向量。

### 1.3 分类标签

​	分类标签一个重要的作用，就是计算预测标签和真实标签之间的相似性，从而计算loss值，loss值越小就说明预测标签和真实标签越接近。

​	举个例子，在神经网络中，需要用一种数学方法表示人，猫，狗的分类，那如何表示呢？最容易想到的就是分别用0,1,2代表人，猫，狗，这种方法肯定是不行的。原因就是，相似性就是两个标签之间的距离，如果按照上述方法，人猫，猫狗之间的距离为1，人狗之间的距离为2，这在参与损失计算的时候是不能接受的，互相独立的标签之间，竟然出现了不对等的情况。

​	所以就需要一种方法，将互相独立的标签表示为互相独立的数字，且数字之间的距离也是相等的。所以one-hot编码就出现了。

## 2. 如何做one-hot编码

​	继续上面的例子，将所有出现的结果放到一个无序且不重复的set里，然后把所有结果作为列，然后每个元素都标记为embedding的形式，比如：有一张表存储了用户属性

| 城市 | 性别 | 年龄段 |
| ---- | ---- | ------ |
| 北京 | 男   | 中年   |
| 深圳 | 女   | 青年   |
| 广州 | 男   | 老年   |
| 北京 | 女   | 青年   |

​	one-hot编码后，就变成了：

| 北京 | 深圳 | 广州 | 男   | 女   | 中年 | 青年 | 老年 |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1    | 0    | 0    | 1    | 0    | 1    | 0    | 0    |
| 0    | 1    | 0    | 0    | 1    | 0    | 1    | 0    |
| 0    | 0    | 1    | 1    | 0    | 0    | 0    | 1    |
| 1    | 0    | 0    | 0    | 1    | 0    | 1    | 0    |



## 3. 如何用one-hot编码

​	在了解了独热编码是怎么做的之后，那么在神经网络中，它是怎么起作用的呢？

​	看过[Softmax 的底层原理](https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzAwOTc2NDU3OQ%3D%3D%26mid%3D2649036706%26idx%3D1%26sn%3D0fbe3fba32fb004d48c031a8fd7eb96c%26chksm%3D834b1c2eb43c9538269b1a8d44acaef7bf9553f8e84806a002431951aff2e803c8d433dff6e7%26scene%3D21%23wechat_redirect)的同学可能还有印象，softmax [激活函数](https://www.zhihu.com/search?q=激活函数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"634296763"})输出一系列概率值，来说明本轮推理属于某一分类的概率是多少。

假设某一轮推理得到的softmax得分如下，有70%的概率是猫，20%的概率是狗，10%的概率是人。

| 分类 | Softmax得分 |
| ---- | ----------- |
| 猫   | 0.7         |
| 狗   | 0.2         |
| 人   | 0.1         |

对于这一轮的预测结果，需要和真实标签进行对比以获取loss值，假设真实标签就是猫。

而猫的独热编码是[1, 0, 0]。

![image-20240812212054949](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240812212054949.png)

​	为了让最终loss值最小，会根据真实标签和预测得分值来调整权重，使得预测得分朝着理想的真实标签靠近。上图中，猫的得分是0.7，而真实得分是1，因此这一类的得分还需要继续增大，计算完loss值后，[反向传播](https://www.zhihu.com/search?q=反向传播&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"634296763"})调整权重，使这一类的得分继续增大。狗和人的得分相反，得分需要继续减小，计算完loss值后，反向传播调整权重，使这一类的得分继续减少。如果预测得分为[1, 0, 0]，那么就和真实标签完全相同，此时的loss值就为0， 就说明本轮训练拟合的很好了。当然one-hot编码有它的局限性，上面的例子是3分类的例子，那如果分类数量有1万个，我们是不是需要将1万个离散的分类，编码成1万维的向量来计算呢？

这就出现了[维度灾难](https://www.zhihu.com/search?q=维度灾难&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"634296763"})了，而且大量的数据为0，向量稀疏的厉害，此时就需要用其他的优化手段来处理了。