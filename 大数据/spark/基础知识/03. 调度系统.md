# 调度系统

## 调度系统主要组件

​	在进程模型的笔记中提到，driver的作用就是解析用户代码，构建计算流图，然后将计算流图拆解成分布式任务，分发到 executor 上执行，这其中需要几个及其重要的组件来协助工作。主要是DAGScheduler，SchedulerBackend 以及 TaskScheduler。

## 调度系统步骤

1. DAGScheduler 以 Shuffle 为边界，将开发者设计的计算图 DAG 拆分为多个执行阶段 Stages，然后为每个 Stage 创建任务集 TaskSet。

2. SchedulerBackend 通过与 Executors 中的 ExecutorBackend 的交互来实时地获取集群中可用的计算资源，并将这些信息记录到 ExecutorDataMap 数据结构。

3. 与此同时，SchedulerBackend 根据 ExecutorDataMap 中可用资源创建 WorkerOffer，以 WorkerOffer 为粒度提供计算资源。

4. 对于给定 WorkerOffer，TaskScheduler 结合 TaskSet 中任务的本地性倾向，按照 PROCESS_LOCAL、NODE_LOCAL、RACK_LOCAL 和 ANY 的顺序，依次对 TaskSet 中的任务进行遍历，优先调度本地性倾向要求苛刻的 Task。

5. 被选中的 Task 由 TaskScheduler 传递给 SchedulerBackend，再由 SchedulerBackend 分发到 Executors 中的 ExecutorBackend。Executors 接收到 Task 之后，即调用本地线程池来执行分布式任务。

    ![img](https://static001.geekbang.org/resource/image/e7/1e/e743cf687bb02bfec8c6ffebbe7d4a1e.jpg?wh=1452x806)

    ![img](https://static001.geekbang.org/resource/image/43/7a/43cff78db9dfc211bb7b15b1c0ea6e7a.jpg?wh=1587x572)

## 组件作用

### DAGScheduler

​	DAGScheduler 的主要职责有三个：

	1. 根据用户提交的 RDD 转换操作或 Spark SQL 查询，构建一个有向无环图（DAG）。该 DAG 描述了从数据源到结果的完整计算过程，反映了 RDD 之间的依赖关系。
	1. 通过分析 RDD 之间的依赖关系，以 `shuffle` 操作为边界，将 DAG 划分为多个阶段（Stages）。它会从最后的行动操作开始，向前递归地分析依赖关系，确保每个 Stage 包含可以并行执行的操作，而不会跨越 `shuffle` 边界。
	1. 每个 Stage 会被进一步划分为多个 Task，`DAGScheduler` 会将这些 Task 打包为一个 `TaskSet`。`TaskSet` 中的每个 Task 负责处理一个数据分区。然后，`DAGScheduler` 会将 `TaskSet` 提交给 `TaskScheduler`，由 `TaskScheduler` 将这些 Task 分配给集群中的 Executor 节点执行。

---

​	以下面的例子来讲，首先根据RDD转换操作构建DAG，在遇到action算子的时候，触发计算，从后向前，以shuffle为边界，以递归的方式，按照DAG创建stages。

​	例子中遇到take触发操作，从后往前找，直到遇到reduceByKey，因为reduceByKey引入shuffle操作，至此第一个stage创建完毕，只包含wordCounts一个RDD。继续向前回溯，直到终点未曾遇到引入shuffle的算子，创建第二个stage，包含沿途所有的RDD。

​	在stages创建完毕之后，依然是按照从后向前的顺序，递归提请执行stage，在提交stage1的时候发现父节点stage0尚未执行，此时把stage1压栈，提请执行stage0，当stage1执行完毕后，通过出栈动作再次提请执行stage1。

![img](https://static001.geekbang.org/resource/image/6f/7b/6f82b4a35cdfb526d837d23675yy477b.jpg?wh=1920x472)

![img](https://static001.geekbang.org/resource/image/24/1c/249eb09407421838782f2515f09yy01c.jpg?wh=1920x534)

![img](https://static001.geekbang.org/resource/image/61/d3/61f394b4bc31af6847944911032119d3.jpg?wh=1920x503)

​	对于提请执行的每一个 Stage，DAGScheduler 根据 Stage 内 RDD 的 partitions 属性创建分布式任务集合 TaskSet。TaskSet 包含一个又一个分布式任务 Task，RDD 有多少数据分区，TaskSet 就包含多少个 Task。换句话说，Task 与 RDD 的分区，是一一对应的。

​	Task 具体是什么呢？要更好地认识 Task，我们不妨来看看它的关键属性。

![img](https://static001.geekbang.org/resource/image/f6/6b/f69d0f189b666c989679ba4d8f7c856b.jpg?wh=1605x277)	Task对象的重要属性在上表中，stageId、stageAttemptId 标记了 Task 与执行阶段 Stage 的所属关系；taskBinary 则封装了隶属于这个执行阶段的用户代码；partition 就是我们刚刚说的 RDD 数据分区；locs 属性以字符串的形式记录了该任务倾向的计算节点或是 Executor ID。

​	不难发现，taskBinary、partition 和 locs 这三个属性，一起描述了这样一件事情：Task 应该在哪里（locs）为谁（partition）执行什么任务（taskBinary)。

### SchedulerBackend

​	负责管理集群中可用的计算资源。

​	SchedulerBackend 用一个叫做 ExecutorDataMap 的数据结构，来记录每一个计算节点中 Executors 的资源状态。ExecutorDataMap 是一种 HashMap，它的 Key 是标记 Executor 的字符串，Value 是一种叫做 ExecutorData 的数据结构。ExecutorData 用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等等，它相当于是对 Executor 做的“资源画像”。

![img](https://static001.geekbang.org/resource/image/e5/ac/e50055d90d3b1c271e4ab97b1ddee6ac.jpg?wh=1920x729)

​	依赖ExecutorDataMap，对内，SchedulerBackend 可以就 Executor 做“资源画像”；对外，SchedulerBackend 以 WorkerOffer 为粒度提供计算资源。其中，WorkerOffer 封装了 Executor ID、主机地址和 CPU 核数，它用来表示一份可用于调度任务的空闲资源。

​	`WorkerOffer` 是 Spark 中用来表示一个可用资源的描述单元，主要在 **任务调度** 中使用。每当 Spark 需要调度任务时，`SchedulerBackend` 会生成多个 `WorkerOffer`，以此表示集群中各个 Executor 上可以用来执行任务的可用资源。

​	SchedulerBackend 与集群内所有 Executors 中的 ExecutorBackend 保持周期性通信，双方通过 LaunchedExecutor、RemoveExecutor、StatusUpdate 等消息来互通有无、变更可用计算资源，更新ExecutorDataMap的数据，从而达到管理集群资源的目的。

### TaskScheduler

​	面对DAGScehduler提交的task和SchedulerBeckend提供的WorkerOffer，TaskScheduler 是依据什么规则来挑选 Tasks 的呢？

​	答案就是：对于给定的 WorkerOffer，TaskScheduler 是按照任务的本地倾向性，来遴选出 TaskSet 中适合调度的 Tasks。

​	上文提到过，Task 与 RDD 的 partitions 是一一对应的，在创建 Task 的过程中，DAGScheduler 会根据数据分区的物理地址，来为 Task 设置 locs 属性。locs 属性记录了数据分区所在的计算节点、甚至是 Executor 进程 ID。

​	举例来说，当我们调用 textFile API 从 HDFS 文件系统中读取源文件时，Spark 会根据 HDFS NameNode 当中记录的元数据，获取数据分区的存储地址，例如 node0:/rootPath/partition0-replica0，node1:/rootPath/partition0-replica1 和 node2:/rootPath/partition0-replica2。

​	那么，DAGScheduler 在为该数据分区创建 Task0 的时候，会把这些地址中的计算节点记录到 Task0 的 locs 属性。如此一来，当 TaskScheduler 需要调度 Task0 这个分布式任务的时候，根据 Task0 的 locs 属性，它就知道：“Task0 所需处理的数据分区，在节点 node0、node1、node2 上存有副本，因此，如果 WorkOffer 是来自这 3 个节点的计算资源，那对 Task0 来说就是投其所好”。

​	从这个例子我们就能更好地理解，每个任务都是自带本地倾向性的，换句话说，每个任务都有自己的“调度意愿”。像上面这种定向到计算节点粒度的本地性倾向，Spark 中的术语叫做 NODE_LOCAL。除了定向到节点，Task 还可以定向到进程（Executor）、机架、任意地址，它们对应的术语分别是 PROCESS_LOCAL、RACK_LOCAL 和 ANY。对于倾向 PROCESS_LOCAL 的 Task 来说，它要求对应的数据分区在某个进程（Executor）中存有副本；而对于倾向 RACK_LOCAL 的 Task 来说，它仅要求相应的数据分区存在于同一机架即可。ANY 则等同于无定向，也就是 Task 对于分发的目的地没有倾向性，被调度到哪里都可以。

下图展示的是，TaskScheduler 依据本地性倾向，依次进行任务调度的运行逻辑：

![img](https://static001.geekbang.org/resource/image/49/6b/495d8ebf85758b4ba5daa5e562da736b.jpg?wh=1920x695)

​	不难发现，从 PROCESS_LOCAL、NODE_LOCAL、到 RACK_LOCAL、再到 ANY，Task 的本地性倾向逐渐从严苛变得宽松。TaskScheduler 接收到 WorkerOffer 之后，也正是按照这个顺序来遍历 TaskSet 中的 Tasks，优先调度本地性倾向为 PROCESS_LOCAL 的 Task，而 NODE_LOCAL 次之，RACK_LOCAL 为再次，最后是 ANY。

​	Spark 调度系统的核心思想，是“数据不动、代码动”。也就是说，在任务调度的过程中，为了完成分布式计算，Spark 倾向于让数据待在原地、保持不动，而把计算任务（代码）调度、分发到数据所在的地方，从而消除数据分发引入的性能隐患。毕竟，相比分发数据，分发代码要轻量得多。本地性倾向则意味着代码和数据应该在哪里“相会”，PROCESS_LOCAL 是在 JVM 进程中，NODE_LOCAL 是在节点内，RACK_LOCAL 是不超出物理机架的范围，而 ANY 则代表“无所谓、不重要”。

### ExecutorBackend	

​	TaskScheduler 就把这些 Tasks 通过 LaunchTask 消息，发送给 SchedulerBackend。SchedulerBackend 拿到这些任务之后，同样使用 LaunchTask 消息，把任务进一步下发给ExecutorBackend，ExecutorBackend把任务派发给 Executors 线程池中一个又一个的 CPU 线程，每个线程负责处理一个 Task。

​	每当 Task 处理完毕，这些线程便会通过 ExecutorBackend，向 Driver 端的 SchedulerBackend 发送 StatusUpdate 事件，告知 Task 执行状态。接下来，TaskScheduler 与 SchedulerBackend 通过接力的方式，最终把状态汇报给 DAGScheduler。

![img](https://static001.geekbang.org/resource/image/c9/a9/c92eca7d5de4c72d478183b187322da9.jpg?wh=1920x742)

一个 DAG 会包含多个 Stages，一个 Stage 的结束即宣告下一个 Stage 的开始，而这也是戴格起初将 DAG 划分为 Stages 的意义所在。只有当所有的 Stages 全部调度、执行完毕，才表示一个完整的 Spark 作业宣告结束。