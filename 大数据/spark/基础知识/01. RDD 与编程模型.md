# RDD

1. 什么是 RDD

    ​	RDD 是一种抽象，是 Spark 对于分布式数据集的抽象，用于囊括所有内存和磁盘中的分布式数据实体。

    ​	RDD 与 数据的概念对比：

    ![img](https://static001.geekbang.org/resource/image/71/76/7149ddfb053edfed4397ee27dc09b376.jpg?wh=1369x718)

    

2. 为什么要了解 RDD

    ​	首先，RDD 是构建 Spark分布式内存计算引擎的基石，很多 Spark 核心概念及核心组件如 DAG 都衍生自 RDD。尽管 RDD API 的使用频率越来越低，但是无论采用哪种 API 或者开发语言，Spark内部最终都会转化为 RDD 上的分布式计算。

3. RDD 重要属性

    - partitions：数据分片
    - partitioner：分片切割规则
    - dependencies： RDD 依赖
    - compute： 转换函数

4. 生活化类比

    ![img](https://static001.geekbang.org/resource/image/4f/da/4fc5769e03f68eae79ea92fbb4756bda.jpg?wh=1920x586)

    ​	起初，工人们从麻袋中把“带泥土豆”加载到流水线，这些土豆经过清洗之后，摇身一变，成了“干净土豆”。

    ​	接下来，流水线上的切片机再把“干净土豆”切成“土豆片”，然后紧接着把这些土豆片放进烤箱。

    ​	最终，土豆片烤熟之后，就变成了可以放心食用的即食薯片。

    ​	通过分析我们不难发现，不同食材形态之间的转换过程，与 Word Count 中不同 RDD 之间的转换过程如出一辙。所以接下来，我们就结合薯片的制作流程，去理解 RDD 的 4 大属性。

    ​	首先，咱们沿着纵向，也就是从上到下的方向，去观察上图中土豆工坊的制作工艺。我们可以看到对于每一种食材形态来说，流水线上都有多个实物与之对应，比如，“带泥土豆”是一种食材形态，流水线上总共有 3 颗“脏兮兮”的土豆同属于这一形态。如果把“带泥土豆”看成是 RDD 的话，那么 RDD 的 partitions 属性，囊括的正是麻袋里那一颗颗脏兮兮的土豆。同理，流水线上所有洗净的土豆，一同构成了“干净土豆”RDD 的 partitions 属性。

    ​	我们再来看 RDD 的 partitioner 属性，这个属性定义了把原始数据集切割成数据分片的切割规则。在土豆工坊的例子中，“带泥土豆”RDD 的切割规则是随机拿取，也就是从麻袋中随机拿取一颗脏兮兮的土豆放到流水线上。后面的食材形态，如“干净土豆”、“土豆片”和“即食薯片”，则沿用了“带泥土豆”RDD 的切割规则。换句话说，后续的这些 RDD，分别继承了前一个 RDD 的 partitioner 属性。这里面与众不同的是“分发的即食薯片”。显然，“分发的即食薯片”是通过对“即食薯片”按照大、中、小号做分发得到的。也就是说，对于“分发的即食薯片”来说，它的 partitioner 属性，重新定义了这个 RDD 数据分片的切割规则，也就是把先前 RDD 的数据分片打散，按照薯片尺寸重新构建数据分片。由这个例子我们可以看出，数据分片的分布，是由 RDD 的 partitioner 决定的。因此，RDD 的 partitions 属性，与它的 partitioner 属性是强相关的。

    ​	横看成岭侧成峰，很多事情换个视角看，相貌可能会完全不同。所以接下来，我们横向地，也就是沿着从左至右的方向，再来观察土豆工坊的制作工艺。不难发现，流水线上的每一种食材形态，都是上一种食材形态在某种操作下进行转换得到的。比如，“土豆片”依赖的食材形态是“干净土豆”，这中间用于转换的操作是“切片”这个动作。

    ​	回顾 Word Count 当中 RDD 之间的转换关系，我们也会发现类似的现象。在数据形态的转换过程中，每个 RDD 都会通过 dependencies 属性来记录它所依赖的前一个、或是多个 RDD，简称“父 RDD”。与此同时，RDD 使用 compute 属性，来记录从父 RDD 到当前 RDD 的转换操作。

    ​	拿 Word Count 当中的 wordRDD 来举例，它的父 RDD 是 lineRDD，因此，它的 dependencies 属性记录的是 lineRDD。从 lineRDD 到 wordRDD 的转换，其所依赖的操作是 flatMap，因此，wordRDD 的 compute 属性，记录的是 flatMap 这个转换函数。	总结下来，薯片的加工流程，与 RDD 的概念和 4 大属性是一一对应的：不同的食材形态，如带泥土豆、土豆片、即食薯片等等，对应的就是 RDD 概念；同一种食材形态在不同流水线上的具体实物，就是 RDD 的 partitions 属性；食材按照什么规则被分配到哪条流水线，对应的就是 RDD 的 partitioner 属性；每一种食材形态都会依赖上一种形态，这种依赖关系对应的是 RDD 中的 dependencies 属性；不同环节的加工方法对应 RDD 的 compute 属性。

    ​	在理解了 RDD 的 4 大属性之后，还需要进一步了解 RDD 的编程模型和延迟计算。编程模型指导我们如何进行代码实现，而延迟计算是 Spark 分布式运行机制的基础。只有搞明白编程模型与延迟计算，你才能流畅地在 Spark 之上做应用开发，在实现业务逻辑的同时，避免埋下性能隐患。

    

# 编程模型与延迟计算

1. RDD 转换

    ​	RDD 是 Spark 对于分布式数据集的抽象，每一个 RDD 都代表着一种分布式数据形态。因此，RDD 到 RDD 之间的转换，本质上是数据形态上的转（Transformations）。

2. 算子类型

    ​	在 RDD 的编程模型中，一共有两种算子，Transformations 类算子和 Actions 类算子。开发者需要使用 Transformations 类算子，定义并描述数据形态的转换过程，然后调用 Actions 类算子，将计算结果收集起来、或是物化到磁盘。

3. 延迟计算

    ​	在这样的编程模型下，Spark 在运行时的计算被划分为两个环节。

    ​		1. 基于不同数据形态之间的转换，构建计算流图（DAG，Directed Acyclic Graph）；

    ​		2. 通过 Actions 类算子，以回溯的方式去触发执行这个计算流图。

    ​	换句话说，开发者调用的各类 Transformations 算子，并不立即执行计算，当且仅当开发者调用 Actions 算子时，之前调用的转换算子才会付诸执行。在业内，这样的计算模式有个专门的术语，叫作“延迟计算”（Lazy Evaluation）。

4. 算子分类

    ![img](https://static001.geekbang.org/resource/image/4f/fa/4f277fdda5a4b34b3e2yyb6f570a08fa.jpg?wh=1773x1364)

    